{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape2d(a):\n",
    "    \"\"\"\n",
    "    Ensures a 2D shape.\n",
    "    Args:\n",
    "        a: a int or tuple/list of length 2\n",
    "    Returns:\n",
    "        list: of length 2. if ``a`` is a int, return ``[a, a]``.\n",
    "    \"\"\"\n",
    "    if type(a) == int:\n",
    "        return [a, a]\n",
    "    if isinstance(a, (list, tuple)):\n",
    "        assert len(a) == 2\n",
    "        return list(a)\n",
    "    raise RuntimeError(\"Illegal shape: {}\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_relu(in_tensor, name=None):\n",
    "    \"\"\"\n",
    "    Shorthand for Batch Normalization + ReLU.\n",
    "    Args:\n",
    "        in_tensor: input tensor\n",
    "        name: name or scope of the variable\n",
    "    Returns:\n",
    "        normalised plus relu activated tensor\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = 'bn'\n",
    "    else:\n",
    "        name = name + '/bn'\n",
    "        \n",
    "    x = layers.BatchNormalization(axis=3, epsilon=1.0e-5, name=name)(in_tensor)\n",
    "    return layers.Activation('relu', name='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_layers(in_tensor):\n",
    "    weights = tf.get_variable(\n",
    "                'kernel', kernel_size, dtype=inputs_dtype, initializer=kernel_initializer)\n",
    "            \n",
    "    # split into sub-tensors and perform group convolution\n",
    "    in_tensor = tf.split(in_tensor, split, -1)\n",
    "    #in_tensor = Lambda(lambda x: tf.split(x, split, -1))(in_tensor)\n",
    "    weights_ = tf.split(weights, split, -1)\n",
    "    #weights_ = Lambda(lambda x: tf.split(x, split, -1))(weights)\n",
    "    \n",
    "    outputs = [tf.nn.conv2d(i, k, strides=strides, padding=padding)\n",
    "                       for i, k in zip(in_tensor, weights_)]\n",
    "    x = tf.concat(outputs, -1)\n",
    "    \n",
    "    if use_bias:\n",
    "            # add bias term\n",
    "            biases = tf.get_variable(\n",
    "                'bias', [kernel_size[-1]], dtype=inputs_dtype, initializer=bias_initializer)\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bKerasLyr=True\n",
    "\n",
    "def conv2d(\n",
    "        name,\n",
    "        in_tensor,\n",
    "        ch,\n",
    "        kernel_size,\n",
    "        strides=1,\n",
    "        padding='SAME',\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=None,\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        split=1):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution operation\n",
    "    Args:\n",
    "        name: name or scope of the variable\n",
    "        in_tensor: input tensor to convolution\n",
    "        ch: number of channels in the input\n",
    "        kernel_size: size of kernel. Assumes square kernel\n",
    "        strides: number of pixels to translate kernel during convolution\n",
    "        padding: 'SAME' or 'VALID' padding\n",
    "        activation (string): activation function to apply to output of convolution\n",
    "        use_bias: whether to add bias to output of convolution (Bool)\n",
    "        kernel_initializer: method to initialise weights for convolution\n",
    "        bias_initializer: method to initialise weights for bias term\n",
    "        split: number of sub-tensors for group convolution (if 1, then normal convolution is used)\n",
    "    Returns:\n",
    "        Tensor output of 2d convolution, with optional bias and activation\n",
    "    \"\"\"\n",
    "    if split == 1:\n",
    "        # use tf.keras.layers.Conv2D is performing stanard convolution\n",
    "        x = layers.Conv2D(ch, kernel_size, strides=strides, padding=padding, use_bias=use_bias, name=name)(in_tensor)\n",
    "    else:\n",
    "        with tf.variable_scope(name):\n",
    "            # convert strides to the form used by tf.nn.conv2d\n",
    "            if strides is None or strides == 1:\n",
    "                strides = [1, 1, 1, 1]\n",
    "            elif strides == 2:\n",
    "                strides = [1, 2, 2, 1]\n",
    "\n",
    "            # shape of the input tensor\n",
    "            inputs_shape = in_tensor.get_shape().as_list()\n",
    "\n",
    "            # convert kernel size to the form used by tf.nn.conv2d\n",
    "            kernel_size = [kernel_size, kernel_size] + [int(inputs_shape[-1]/split), ch]\n",
    "            inputs_dtype = in_tensor.dtype\n",
    "            \n",
    "            if bKerasLyr==False:\n",
    "                weights = tf.get_variable(\n",
    "                    'kernel', kernel_size, dtype=inputs_dtype, initializer=kernel_initializer)\n",
    "\n",
    "                # split into sub-tensors and perform group convolution\n",
    "                in_tensor = tf.split(in_tensor, split, -1)\n",
    "                #in_tensor = Lambda(lambda x: tf.split(x, split, -1))(in_tensor)\n",
    "                weights_ = tf.split(weights, split, -1)\n",
    "                #weights_ = Lambda(lambda x: tf.split(x, split, -1))(weights)\n",
    "                outputs = [tf.nn.conv2d(i, k, strides=strides, padding=padding)\n",
    "                           for i, k in zip(in_tensor, weights_)]\n",
    "                x = tf.concat(outputs, -1)\n",
    "            else:\n",
    "                x=Lambda(split_layers)(in_tensor)\n",
    "\n",
    "        if use_bias:\n",
    "            # add bias term\n",
    "            biases = tf.get_variable(\n",
    "                'bias', [kernel_size[-1]], dtype=inputs_dtype, initializer=bias_initializer)\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "\n",
    "    if activation is not None:\n",
    "        # apply activation function\n",
    "        if activation == 'bnrelu':\n",
    "            x = batch_norm_relu(x, name=name)\n",
    "        else:\n",
    "            x = layers.Activation(activation)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample2x(in_tensor, shape=2, unpool_mat=np.ones((2, 2), dtype='float32')):\n",
    "    \"\"\"\n",
    "    Up sample the input with a fixed matrix to perform kronecker product with.\n",
    "    Args:\n",
    "        in_tensor: input tensor\n",
    "        shape: int or (h, w) tuple\n",
    "        unpool_mat: a tf.Tensor or np.ndarray 2D matrix with size=shape.\n",
    "                    If is None, will use a matrix with 1 at top-left corner\n",
    "    Returns:\n",
    "        2x UpSampled tensor via unpooling\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = in_tensor.get_shape().as_list()\n",
    "    shape = shape2d(shape)\n",
    "\n",
    "    # check unpool_mat\n",
    "    if unpool_mat is None:\n",
    "        mat = np.zeros(shape, dtype='float32')\n",
    "        mat[0][0] = 1\n",
    "        unpool_mat = tf.constant(mat, name='unpool_mat')\n",
    "    elif isinstance(unpool_mat, np.ndarray):\n",
    "        unpool_mat = tf.constant(unpool_mat, name='unpool_mat')\n",
    "    assert unpool_mat.shape.as_list() == list(shape)  # check as_list() works for tuple, I am getting a warning?\n",
    "\n",
    "    x = tf.transpose(in_tensor, [0, 3, 1, 2])\n",
    "\n",
    "    # perform a tensor-matrix kronecker product\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    mat = tf.expand_dims(unpool_mat, 0)\n",
    "    x = tf.tensordot(x, mat, axes=1)\n",
    "    x = tf.transpose(x, [0, 2, 4, 3, 5, 1])\n",
    "\n",
    "    return tf.reshape(x, [-1, shape[0]*input_shape[1], shape[1]*input_shape[2], input_shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_op(x, cropping, data_format='channels_first'):\n",
    "    \"\"\"\n",
    "    Center crop image\n",
    "    Args:\n",
    "        cropping is the substracted portion\n",
    "    \"\"\"\n",
    "    crop_t = cropping[0] // 2\n",
    "    crop_b = cropping[0] - crop_t\n",
    "    crop_l = cropping[1] // 2\n",
    "    crop_r = cropping[1] - crop_l\n",
    "    if data_format == 'channels_first':\n",
    "        x = x[:,:,crop_t:-crop_b,crop_l:-crop_r]\n",
    "    else:\n",
    "        x = x[:,crop_t:-crop_b,crop_l:-crop_r]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_blk(name, in_tensor, ch, kernel_size, count, strides=1):\n",
    "    \"\"\"\n",
    "    Residual block consisting of <count> residual units.\n",
    "    He, Kaiming, et al. \"Deep residual learning for image recognition.\" \n",
    "    Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "    Args:\n",
    "        name: variable scope name\n",
    "        in_tensor: input tensor\n",
    "        ch: number of output channels\n",
    "        kernel_size: kernel size\n",
    "        count: number of residual units\n",
    "        strides: strides of second convolution\n",
    "    Returns:\n",
    "        out_tensor: output of residual block\n",
    "    \"\"\"\n",
    "    ch_in = in_tensor.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        for i in range(0, count):\n",
    "            with tf.variable_scope('block' + str(i)):\n",
    "                x = in_tensor if i == 0 else batch_norm_relu(in_tensor, 'preact')\n",
    "                x = conv2d('conv1', x, ch[0], kernel_size[0], activation='bnrelu')\n",
    "                x = conv2d('conv2', x, ch[1], kernel_size[1],\n",
    "                           strides=strides if i == 0 else 1, activation='bnrelu')\n",
    "                x = conv2d('conv3', x, ch[2], kernel_size[2])\n",
    "                if (strides != 1 or ch_in[-1] != ch[2]) and i == 0:\n",
    "                    in_tensor = conv2d('convshortcut', in_tensor, ch[2], 1, strides=strides)\n",
    "                in_tensor = in_tensor + x\n",
    "        # end of each group need an extra activation\n",
    "        out_tensor = batch_norm_relu(in_tensor, 'bnlast')\n",
    "    return out_tensor\n",
    "####\n",
    "\n",
    "\n",
    "def dense_blk(name, in_tensor, ch, kernel_size, count, split, padding='VALID'): \n",
    "    \"\"\"\n",
    "    Dense block consisting of <count> dense units.\n",
    "    Huang, Gao, et al. \"Densely connected convolutional networks.\" \n",
    "    Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
    "    Args:\n",
    "        name: variable scope name\n",
    "        in_tensor: input tensor\n",
    "        ch: number of channels\n",
    "        kernel_size: kernel size\n",
    "        count: number of dense units within block\n",
    "        split: number of sub tensors if using group convolution\n",
    "        padding: type of padding- choose 'SAME' or 'VALID'.\n",
    "    Returns:\n",
    "        out_tensor:\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        for i in range(0, count):\n",
    "            with tf.variable_scope('blk/' + str(i)):\n",
    "                x = batch_norm_relu(in_tensor, 'preact_bna')\n",
    "                x = conv2d('conv1', x, ch[0], kernel_size[0],\n",
    "                           padding=padding, activation='bnrelu')\n",
    "                x = conv2d('conv2', x, ch[1], kernel_size[1], padding=padding, split=split)\n",
    "                ##\n",
    "                if padding == 'VALID':\n",
    "                    x_shape = x.get_shape().as_list()\n",
    "                    in_shape = in_tensor.get_shape().as_list()\n",
    "                    in_tensor = crop_op(in_tensor, (in_shape[1] - x_shape[1], in_shape[2] - x_shape[2]))\n",
    "\n",
    "                in_tensor = tf.concat([in_tensor, x], axis=-1)\n",
    "        out_tensor = batch_norm_relu(in_tensor, 'blk_bna')\n",
    "    return out_tensor\n",
    "####\n",
    "\n",
    "\n",
    "def encoder(in_tensor, input_pad):\n",
    "    \"\"\"\n",
    "    Pre-activated ResNet50 Encoder\n",
    "    Args:\n",
    "        in_tensor: input tensor\n",
    "        input_pad: type of padding for first convolution- 'SAME' or 'VALID'\n",
    "    Returns:\n",
    "        [d1, d2, d3, d4]: ResNet50 block outputs\n",
    "    \"\"\"\n",
    "    d1 = conv2d('conv0', in_tensor, 64, 7, padding=input_pad, strides=1, activation='bnrelu')\n",
    "    d1 = res_blk('group0', d1, [64,  64,   256], [1, 3, 1], 3, strides=1)\n",
    "    d2 = res_blk('group1', d1, [128, 128,  512], [1, 3, 1], 4, strides=2)\n",
    "    d3 = res_blk('group2', d2, [256, 256, 1024], [1, 3, 1], 6, strides=2)\n",
    "    d4 = res_blk('group3', d3, [512, 512, 2048], [1, 3, 1], 3, strides=2)\n",
    "    d4 = conv2d('conv_bot', d4, 1024, 1, padding='SAME')\n",
    "    return [d1, d2, d3, d4]\n",
    "####\n",
    "\n",
    "\n",
    "def decoder(name, in_tensor, ksize):\n",
    "    \"\"\"\n",
    "    Dense decoder unit. \n",
    "    Upsamples ResNet50 features by using a combination \n",
    "    of upsampling operations and dense blocks. \n",
    "    Args:\n",
    "        name: variable scope name\n",
    "        in_tensor: input tensor\n",
    "        ksize: kernel size to use in decoder\n",
    "    Returns:\n",
    "        [u3, u2x, u1]:\n",
    "    \"\"\"\n",
    "    pad = 'VALID'  # to prevent boundary artefacts\n",
    "    with tf.variable_scope(name):\n",
    "        with tf.variable_scope('u3'):\n",
    "            u3 = Lambda(upsample2x)(in_tensor[-1])\n",
    "\n",
    "            # skip connection\n",
    "            u3_skip = in_tensor[-2]\n",
    "            u3_sum = tf.add_n([u3, u3_skip])\n",
    "\n",
    "            u3 = conv2d('conva', u3_sum, 256, ksize, strides=1, padding=pad)\n",
    "            u3 = dense_blk('dense', u3, [128, 32], [1, ksize], 8, split=4, padding=pad)\n",
    "            u3 = conv2d('convf', u3, 512, 1, strides=1)\n",
    "        ####\n",
    "        with tf.variable_scope('u2'):          \n",
    "            u2 = Lambda(upsample2x)(u3)\n",
    "\n",
    "            # skip connection\n",
    "            u2_skip = in_tensor[-3]\n",
    "            u2_shape = u2.get_shape().as_list()\n",
    "            u2_skip_shape = u2_skip.get_shape().as_list()\n",
    "            u2_sum = tf.add_n([u2, crop_op(\n",
    "                u2_skip, (u2_skip_shape[1]-u2_shape[1], u2_skip_shape[2]-u2_shape[2]))])\n",
    "\n",
    "            u2x = conv2d('conva', u2_sum, 128, ksize, strides=1, padding=pad)\n",
    "            u2 = dense_blk('dense', u2x, [128, 32], [1, ksize], 4, split=4, padding=pad)\n",
    "            u2 = conv2d('convf', u2, 256, 1, strides=1)\n",
    "        ####\n",
    "        with tf.variable_scope('u1'):          \n",
    "            u1 = Lambda(upsample2x)(u2)\n",
    "\n",
    "            # skip connection\n",
    "            u1_skip = in_tensor[-4]\n",
    "            u1_shape = u1.get_shape().as_list()\n",
    "            u1_skip_shape = u1_skip.get_shape().as_list()\n",
    "            u1_sum = tf.add_n([u1, crop_op(\n",
    "                u1_skip, (u1_skip_shape[1]-u1_shape[1], u1_skip_shape[2]-u1_shape[2]))])\n",
    "\n",
    "            u1 = conv2d('conva', u1_sum, 64, ksize, strides=1, padding='SAME')\n",
    "\n",
    "    return [u3, u2x, u1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(network, is_training, images=None, decoder_ksize=5, num_of_classes=5, input_pad='VALID'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        network: input Model class\n",
    "        is_training: True if training the network otherwise False\n",
    "        images: input images in NHWC format\n",
    "        decoder_ksize: kernel size for decoders \n",
    "        num_of_classes: number of classes at output of NC branch\n",
    "        input_pad: type of padding for first convolution ('SAME' or 'VALID')\n",
    "    Returns:\n",
    "        logi_nc: Nuclei classification logits\n",
    "        logi_np: Nuclei Pixels logits\n",
    "        logi_hv: Horizontal Vertical logits\n",
    "    \"\"\"\n",
    "    print(network.batch_size)\n",
    "    print(is_training)\n",
    "\n",
    "    images = images/255.0  # normalise input between 0 and 1\n",
    "\n",
    "    ####\n",
    "    d = encoder(images, input_pad)\n",
    "\n",
    "    ####\n",
    "    np_feat = decoder('np', d, decoder_ksize)\n",
    "    npx = batch_norm_relu(np_feat[-1], 'preact_out_np')\n",
    "\n",
    "    hv_feat = decoder('hv', d, decoder_ksize)\n",
    "    hv = batch_norm_relu(hv_feat[-1], 'preact_out_hv')\n",
    "\n",
    "    tp_feat = decoder('tp', d, decoder_ksize)\n",
    "    tp = batch_norm_relu(tp_feat[-1], 'preact_out_tp')\n",
    "\n",
    "    # Nuclei Type Pixels (TP)\n",
    "    logi_nc = conv2d('conv_out_tp', tp, num_of_classes, 1, use_bias=True)\n",
    "    logi_nc = layers.Softmax(axis=-1, name='softmax')(logi_nc)\n",
    "\n",
    "    # Nuclei Pixels (NP)\n",
    "    logi_np = conv2d('conv_out_np', npx, 2, 1, use_bias=True)\n",
    "    logi_np = layers.Softmax(axis=-1, name='softmax')(logi_np)\n",
    "\n",
    "    # Horizontal-Vertical (HV)\n",
    "    logi_hv = conv2d('conv_out_hv', hv, 2, 1, use_bias=True)\n",
    "\n",
    "    return logi_nc, logi_np, logi_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "input_layer=layers.Input(shape=(270,270,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " d = encoder(input_layer, 'VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kernel_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c0445b0c4dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdecoder_ksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'np'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_ksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnpx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_feat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'preact_out_np'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-a08b78d5e651>\u001b[0m in \u001b[0;36mdecoder\u001b[1;34m(name, in_tensor, ksize)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mu3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conva'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu3_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mu3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdense_blk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dense'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mu3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'convf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-a08b78d5e651>\u001b[0m in \u001b[0;36mdense_blk\u001b[1;34m(name, in_tensor, ch, kernel_size, count, split, padding)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 x = conv2d('conv1', x, ch[0], kernel_size[0],\n\u001b[0;32m     55\u001b[0m                            padding=padding, activation='bnrelu')\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'VALID'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-2e9995106260>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(name, in_tensor, ch, kernel_size, strides, padding, activation, use_bias, kernel_initializer, bias_initializer, split)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hovernet\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hovernet\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mask'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e421b21913c1>\u001b[0m in \u001b[0;36msplit_layers\u001b[1;34m(in_tensor)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     weights = tf.get_variable(\n\u001b[1;32m----> 3\u001b[1;33m                 'kernel', kernel_size, dtype=inputs_dtype, initializer=kernel_initializer)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# split into sub-tensors and perform group convolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kernel_size' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_ksize=5\n",
    "\n",
    "np_feat = decoder('np', d, decoder_ksize)\n",
    "npx = batch_norm_relu(np_feat[-1], 'preact_out_np')\n",
    "\n",
    "hv_feat = decoder('hv', d, decoder_ksize)\n",
    "hv = batch_norm_relu(hv_feat[-1], 'preact_out_hv')\n",
    "\n",
    "tp_feat = decoder('tp', d, decoder_ksize)\n",
    "tp = batch_norm_relu(tp_feat[-1], 'preact_out_tp')\n",
    "\n",
    "# Nuclei Type Pixels (TP)\n",
    "logi_nc = conv2d('conv_out_tp', tp, num_of_classes, 1, use_bias=True)\n",
    "logi_nc = layers.Softmax(axis=-1, name='softmax')(logi_nc)\n",
    "\n",
    "# Nuclei Pixels (NP)\n",
    "logi_np = conv2d('conv_out_np', npx, 2, 1, use_bias=True)\n",
    "logi_np = layers.Softmax(axis=-1, name='softmax')(logi_np)\n",
    "\n",
    "# Horizontal-Vertical (HV)\n",
    "logi_hv = conv2d('conv_out_hv', hv, 2, 1, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (hovernet)",
   "language": "python",
   "name": "hovernet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
